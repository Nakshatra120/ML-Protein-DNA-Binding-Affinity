{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29620603",
   "metadata": {},
   "source": [
    "# Myc/Max — Random Forest Reproducible Notebook\n",
    "\n",
    "This notebook reproduces Random Forest (RF) results and figures for the Myc/Max TF–DNA binding project. It follows the exact CLI workflow used in your repo (`torch_prep_kfold.py` → `run_model.py`) and adds notebook-native plots for:\n",
    "- Regression scatter vs. experimental ΔΔG_bind\n",
    "- Binary confusion matrix (from a threshold on ΔΔG_bind)\n",
    "- Feature importance (Gini and permutation)\n",
    "\n",
    "**Before you run:**\n",
    "1) Place your repository scripts at the project root (same directory structure as your docs):\n",
    "```\n",
    "project_root/\n",
    "├─ Inputs/                 # you provide\n",
    "│   ├─ REFERENCE.csv       # contains SEQUENCE_ID and LABEL_COL\n",
    "│   └─ FEATURE*.csv        # 8 MMGBSA components (and corrections) averaged by frame\n",
    "├─ Data/                   # will be created/used by scripts\n",
    "├─ Model/                  # models saved here\n",
    "├─ torch_prep_kfold.py\n",
    "├─ run_model.py\n",
    "└─ hyperopt.py             # optional\n",
    "```\n",
    "2) Activate the correct env (pandas, numpy, scikit-learn, scipy, matplotlib, seaborn optional).\n",
    "3) Update the **CONFIG** cell below to your actual column names and filenames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9199aa76",
   "metadata": {},
   "source": [
    "## CONFIG — set your paths and identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90347771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>> EDIT THESE <<<<<\n",
    "PROJECT_ROOT = \".\"\n",
    "DATA_DIR = f\"{PROJECT_ROOT}/Data\"\n",
    "MODEL_DIR = f\"{PROJECT_ROOT}/Model\"\n",
    "INPUTS_DIR = f\"{PROJECT_ROOT}/Inputs\"\n",
    "\n",
    "# ID + label columns\n",
    "SEQUENCE_ID = \"sequence\"      # e.g., 'sequence'\n",
    "LABEL_COL   = \"bind_avg\"      # e.g., 'bind_avg' for regression\n",
    "\n",
    "# Feature CSVs (provide one or more, joined on SEQUENCE_ID)\n",
    "FEATURE_FILES = [\n",
    "    f\"{INPUTS_DIR}/FEATURES_MMGBSA.csv\"\n",
    "    # add more if you split per-term CSVs\n",
    "]\n",
    "\n",
    "REFERENCE_FILE = f\"{INPUTS_DIR}/REFERENCE.csv\"   # contains SEQUENCE_ID + LABEL_COL\n",
    "\n",
    "# Split + CV knobs (align with best settings)\n",
    "TEST_PERCENT = 0.15\n",
    "KFOLD = 5\n",
    "NUM_REPEATS = 1\n",
    "KEEP_LAST_PERCENT = 0       # 0 = keep all\n",
    "NAVG = 50                   # average rows per sequence in chunks\n",
    "SCR_FRACTIONS = \"0.0\"       # naming uses only the first element\n",
    "\n",
    "# RF hyperparameters (example starting point; update if you have best params)\n",
    "N_ESTIMATORS = 300\n",
    "MAX_DEPTH = None           # or an int (e.g., 10)\n",
    "MAX_FEATURES = \"auto\"      # 'auto', 'sqrt', 0.5, etc.\n",
    "MIN_SAMPLES_SPLIT = 2\n",
    "MIN_SAMPLES_LEAF = 1\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Prefix + model type\n",
    "PREFIX = \"mycmax\"\n",
    "MODEL_TYPE = \"reg\"          # 'reg', 'bin', or 'mclass'\n",
    "\n",
    "# Plot/analysis knobs\n",
    "BINARY_THRESHOLD = 0.0      # threshold on ΔΔG_bind for confusion matrix\n",
    "DATA_SCALE = \"log\"          # if your run_model.py expects 'log' scaling flag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dbc575",
   "metadata": {},
   "source": [
    "## 1) Initial split (merge features + hold-out test by sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fad5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, shlex, pathlib\n",
    "\n",
    "pathlib.Path(DATA_DIR).mkdir(exist_ok=True, parents=True)\n",
    "pathlib.Path(MODEL_DIR).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Build the CLI for initial split\n",
    "cmd = f\"\"\"python {PROJECT_ROOT}/torch_prep_kfold.py --initial_split   --reference_file {REFERENCE_FILE}   --ref_id_col {SEQUENCE_ID}   --ref_label_col {LABEL_COL}   --filenames {' '.join(FEATURE_FILES)}   --feature_id_col {SEQUENCE_ID}   --prefix {PREFIX} --model_type {MODEL_TYPE}   --test_percentage {TEST_PERCENT}   --scramble_fractions {SCR_FRACTIONS}\n",
    "\"\"\"\n",
    "print(cmd)\n",
    "# subprocess.run(shlex.split(cmd), check=False)  # uncomment to execute when your scripts are available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7b145",
   "metadata": {},
   "source": [
    "## 2) Build repeated K-folds for training (standardize using training stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"\"\"python {PROJECT_ROOT}/torch_prep_kfold.py --process train   --keep_last_percent {KEEP_LAST_PERCENT}   --navg {NAVG}   --kfold {KFOLD} --num_repeats {NUM_REPEATS}   --prefix {PREFIX} --model_type {MODEL_TYPE}   --scramble_fractions {SCR_FRACTIONS}   --random_state {RANDOM_STATE}\n",
    "\"\"\"\n",
    "print(cmd)\n",
    "# subprocess.run(shlex.split(cmd), check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8a5041",
   "metadata": {},
   "source": [
    "## 3) Train RF with cross‑validation (saves models + CV metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca95bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"\"\"python {PROJECT_ROOT}/run_model.py --mode 0   --model_type {MODEL_TYPE} --data_scale {DATA_SCALE}   --kfold {KFOLD} --num_repeats {NUM_REPEATS}   --model_dir {MODEL_DIR} --data_dir {DATA_DIR}   --output_file predictions   --ref_id_col {SEQUENCE_ID} --ref_label_col {LABEL_COL}   --n_estimators {N_ESTIMATORS} --max_depth {str(MAX_DEPTH)} --max_features {MAX_FEATURES}   --min_samples_split {MIN_SAMPLES_SPLIT} --min_samples_leaf {MIN_SAMPLES_LEAF}   --random_state {RANDOM_STATE}   --prefix {PREFIX} --scramble_fractions {SCR_FRACTIONS}\n",
    "\"\"\"\n",
    "print(cmd)\n",
    "# subprocess.run(shlex.split(cmd), check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52409005",
   "metadata": {},
   "source": [
    "## 4) Prepare the test set using the training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e0a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"\"\"python {PROJECT_ROOT}/torch_prep_kfold.py --process test   --keep_last_percent {KEEP_LAST_PERCENT}   --navg {NAVG}   --prefix {PREFIX} --model_type {MODEL_TYPE}   --scramble_fractions {SCR_FRACTIONS}   --random_state {RANDOM_STATE}\n",
    "\"\"\"\n",
    "print(cmd)\n",
    "# subprocess.run(shlex.split(cmd), check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f763a69",
   "metadata": {},
   "source": [
    "## 5) Evaluate saved models on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"\"\"python {PROJECT_ROOT}/run_model.py --mode 1   --model_type {MODEL_TYPE} --data_scale {DATA_SCALE}   --kfold {KFOLD} --num_repeats {NUM_REPEATS}   --model_dir {MODEL_DIR} --data_dir {DATA_DIR}   --output_file predictions   --ref_id_col {SEQUENCE_ID} --ref_label_col {LABEL_COL}   --random_state {RANDOM_STATE}   --prefix {PREFIX} --scramble_fractions {SCR_FRACTIONS}\n",
    "\"\"\"\n",
    "print(cmd)\n",
    "# subprocess.run(shlex.split(cmd), check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d9fbcf",
   "metadata": {},
   "source": [
    "## 6) Notebook-native plots to match the paper/supplement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e9c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Locate outputs produced by the pipeline ----\n",
    "cv_pred_path = Path(DATA_DIR) / f\"predictions_{MODEL_TYPE}_final_avg_scr0p00.csv\"\n",
    "tst_pred_path = Path(DATA_DIR) / f\"predictions_test_{MODEL_TYPE}_scr0p00.csv\"\n",
    "cv_metrics_path = Path(PROJECT_ROOT) / f\"final_metrics_{MODEL_TYPE}_trn_scr0p00.csv\"\n",
    "tst_metrics_path = Path(PROJECT_ROOT) / f\"final_metrics_{MODEL_TYPE}_tst_scr0p00.csv\"\n",
    "\n",
    "print(\"CV predictions:\", cv_pred_path.exists(), cv_pred_path)\n",
    "print(\"Test predictions:\", tst_pred_path.exists(), tst_pred_path)\n",
    "\n",
    "# ---- Load predictions (sequence-level) ----\n",
    "# Expected columns (example): sequence, y_true (experimental ΔΔG), y_pred (model ΔΔG)\n",
    "def _load_preds(path):\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path)\n",
    "        # Best guess for column names; adjust if your files use different names\n",
    "        # Try to infer\n",
    "        cols = df.columns.str.lower()\n",
    "        if 'y_true' not in cols.tolist() or 'y_pred' not in cols.tolist():\n",
    "            print(\"[Note] Please rename columns to include y_true and y_pred, or edit here.\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"[WARN] Missing: {path}\")\n",
    "        return None\n",
    "\n",
    "cv_df = _load_preds(cv_pred_path)\n",
    "tst_df = _load_preds(tst_pred_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fae693",
   "metadata": {},
   "source": [
    "### 6A) Regression scatter (ΔΔG_bind: predicted vs experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(df, title=\"Regression (ΔΔG_bind)\"):\n",
    "    if df is None: \n",
    "        print(\"No dataframe provided.\")\n",
    "        return\n",
    "    # Infer columns\n",
    "    lc = df.columns.str.lower()\n",
    "    y_true_col = df.columns[lc.get_loc('y_true')] if 'y_true' in lc else df.columns[-2]\n",
    "    y_pred_col = df.columns[lc.get_loc('y_pred')] if 'y_pred' in lc else df.columns[-1]\n",
    "    x = df[y_true_col].values\n",
    "    y = df[y_pred_col].values\n",
    "\n",
    "    pcc, _ = pearsonr(x, y)\n",
    "    mse = mean_squared_error(x, y)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(x, y, alpha=0.7)\n",
    "    plt.axhline(0, linestyle='--')\n",
    "    plt.axvline(0, linestyle='--')\n",
    "    plt.title(f\"{title}\\nPearson={pcc:.2f}, MSE={mse:.2f}\")\n",
    "    plt.xlabel(\"Experimental ΔΔG_bind\")\n",
    "    plt.ylabel(\"Predicted ΔΔG_bind\")\n",
    "    plt.show()\n",
    "\n",
    "plot_scatter(cv_df, title=\"Training CV (averaged predictions)\")\n",
    "plot_scatter(tst_df, title=\"Held‑out Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d63ae5",
   "metadata": {},
   "source": [
    "### 6B) Binary confusion matrix (threshold on ΔΔG_bind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd474098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_binary_cm(df, threshold=0.0, title=\"Binary classification (ΔΔG threshold)\"):\n",
    "    if df is None: \n",
    "        print(\"No dataframe provided.\")\n",
    "        return\n",
    "    lc = df.columns.str.lower()\n",
    "    y_true_col = df.columns[lc.get_loc('y_true')] if 'y_true' in lc else df.columns[-2]\n",
    "    y_pred_col = df.columns[lc.get_loc('y_pred')] if 'y_pred' in lc else df.columns[-1]\n",
    "    y_true = (df[y_true_col].values > threshold).astype(int)\n",
    "    y_hat  = (df[y_pred_col].values > threshold).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_hat, labels=[0,1])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"non‑binding\",\"binding\"])\n",
    "    disp.plot(values_format='d')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_binary_cm(cv_df, threshold=BINARY_THRESHOLD, title=\"Training CV (threshold=0.0)\")\n",
    "plot_binary_cm(tst_df, threshold=BINARY_THRESHOLD, title=\"Held‑out Test (threshold=0.0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f084f70",
   "metadata": {},
   "source": [
    "### 6C) Feature importance (Gini + optional permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you saved per‑fold RF models, you can reload and average feature_importances_. \n",
    "# As a simple placeholder, this cell expects a CSV with per‑feature Gini importances if your pipeline produced it.\n",
    "# Otherwise, adapt to load your .pkl models from Model/ and extract .feature_importances_.\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "FEATURE_IMPORTANCE_CSV = Path(DATA_DIR) / \"rf_feature_importances_scr0p00.csv\"  # optional\n",
    "if FEATURE_IMPORTANCE_CSV.exists():\n",
    "    imp = pd.read_csv(FEATURE_IMPORTANCE_CSV)\n",
    "    # Expect columns: feature, gini_importance\n",
    "    imp = imp.sort_values('gini_importance', ascending=False)\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.bar(imp['feature'], imp['gini_importance'])\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel(\"Gini importance\")\n",
    "    plt.title(\"Random Forest — Feature Importance (Gini)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"[Note] {FEATURE_IMPORTANCE_CSV} not found. If you have saved models, load them and compute importances here.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d30eb49",
   "metadata": {},
   "source": [
    "## 7) Reproducibility report — capture config + env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, platform, json, pandas as pd\n",
    "report = {\n",
    "    \"python\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"config\": {\n",
    "        \"PROJECT_ROOT\": PROJECT_ROOT,\n",
    "        \"DATA_DIR\": DATA_DIR,\n",
    "        \"MODEL_DIR\": MODEL_DIR,\n",
    "        \"INPUTS_DIR\": INPUTS_DIR,\n",
    "        \"SEQUENCE_ID\": SEQUENCE_ID,\n",
    "        \"LABEL_COL\": LABEL_COL,\n",
    "        \"TEST_PERCENT\": TEST_PERCENT,\n",
    "        \"KFOLD\": KFOLD,\n",
    "        \"NUM_REPEATS\": NUM_REPEATS,\n",
    "        \"KEEP_LAST_PERCENT\": KEEP_LAST_PERCENT,\n",
    "        \"NAVG\": NAVG,\n",
    "        \"SCR_FRACTIONS\": SCR_FRACTIONS,\n",
    "        \"RF\": {\n",
    "            \"n_estimators\": N_ESTIMATORS,\n",
    "            \"max_depth\": MAX_DEPTH,\n",
    "            \"max_features\": MAX_FEATURES,\n",
    "            \"min_samples_split\": MIN_SAMPLES_SPLIT,\n",
    "            \"min_samples_leaf\": MIN_SAMPLES_LEAF,\n",
    "            \"random_state\": RANDOM_STATE\n",
    "        },\n",
    "        \"MODEL_TYPE\": MODEL_TYPE,\n",
    "        \"PREFIX\": PREFIX,\n",
    "        \"DATA_SCALE\": DATA_SCALE\n",
    "    }\n",
    "}\n",
    "with open(\"/mnt/data/mycmax_rf_repro_config.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(\"Saved config to /mnt/data/mycmax_rf_repro_config.json\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
